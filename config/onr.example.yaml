auth:
  # Optional master key for incoming requests.
  api_key: ""
  token_key:
    # Keep requiring k/k64 in onr:v1 token by default.
    # Set true to allow BYOK token with only uk/uk64.
    allow_byok_without_k: false

server:
  listen: ":3300"
  read_timeout_ms: 60000
  write_timeout_ms: 60000
  # PID file for `onr -s reload` (SIGHUP).
  pid_file: "/var/run/onr.pid"

providers:
  # Directory containing provider DSL files (*.conf)
  dir: "./config/providers"
  auto_reload:
    # Watch providers.dir and reload provider DSL files automatically.
    # Disabled by default; enable only when you need file-watch based reload.
    enabled: false
    # Merge rapid editor events into one reload.
    debounce_ms: 300

keys:
  # Upstream keys file (grouped by provider)
  file: "./keys.yaml"

models:
  # Model routing file (model -> providers)
  file: "./models.yaml"

oauth:
  token_persist:
    # Persist OAuth access tokens to local files (for OAuth-enabled providers).
    enabled: true
    # Directory used when token persistence is enabled.
    dir: "./run/oauth"

pricing:
  # Global switch for runtime cost calculation.
  enabled: false
  # Base price catalog (generated by: onr-admin pricing sync)
  file: "./price.yaml"
  # Optional local overrides for provider/channel multipliers and model price overrides
  overrides_file: "./price_overrides.yaml"

upstream_proxies:
  # Configure outbound HTTP proxy per provider (optional).
  # Example:
  #   by_provider:
  #     openai: "http://127.0.0.1:7890"
  #     anthropic: "http://127.0.0.1:7891"
  # Supported schemes:
  #   - http:// / https://
  #   - socks5:// / socks5h:// (optional user/pass: socks5://user:pass@host:port)
  by_provider:

usage_estimation:
  # Estimate token usage when upstream does not return usage (or returns all zeros).
  # This is best-effort and intended for local debugging / rough observability.
  enabled: true
  estimate_when_missing_or_zero: true
  strategy: "heuristic"
  max_request_bytes: 1048576
  max_response_bytes: 1048576
  max_stream_collect_bytes: 262144
  apis: ["chat.completions", "responses", "claude.messages", "embeddings", "gemini.generateContent", "gemini.streamGenerateContent"]

traffic_dump:
  # Enable traffic dump logs to files (request/response capture, best-effort masking).
  enabled: false
  dir: "./dumps"
  file_path: "{{.request_id}}.log"
  max_bytes: 1048576
  mask_secrets: true

logging:
  level: "info"
  # Access log output (nginx-like):
  # - access_log: enable/disable
  # - access_log_path: if set, append logs to this file; otherwise output to stdout
  # - access_log_format: optional custom format with $variables; non-empty has highest priority
  # - access_log_format_preset: optional preset used only when access_log_format is empty
  #   presets: onr_combined / onr_minimal
  #   Available variables:
  #   $time_local $status $latency $latency_ms $client_ip $method $path
  #   $request_id $appname $provider $provider_source $api $stream $model
  #   $usage_stage $input_tokens $output_tokens $total_tokens
  #   $cache_read_tokens $cache_write_tokens
  #   $cost_total $cost_input $cost_output $cost_cache_read $cost_cache_write
  #   $billable_input_tokens $cost_multiplier $cost_model $cost_channel $cost_unit
  #   $upstream_status $finish_reason $ttft_ms $tps
  # - appname_infer.enabled: infer appname from User-Agent when request header `appname` is missing
  # - appname_infer.unknown: fallback appname when inference misses; empty means omit appname field
  access_log: true
  access_log_path: ""
  access_log_format: ""
  access_log_format_preset: ""
  appname_infer:
    enabled: false
    unknown: ""
